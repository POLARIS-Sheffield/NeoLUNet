import os
import sys
import warnings
import pandas as pd
import numpy as np
from DL_utils.model2D import unet2D
from pydicom import dcmread
from sklearn.cluster import KMeans
from sklearn.preprocessing import scale
from sklearn.svm import LinearSVC
from sklearn.metrics import silhouette_score
from matplotlib import pyplot as plt


def crop_patient_images(image_array, return_only_cuts=False, tol=0.05, target_shape=128):
    """
    :param image_array: raw scan slices from one patient which should be cropped to fit the models' input shape
    :param return_only_cuts: if true, only the cut borders are returned
    :param tol: tolerance relatively to the brightest pixel. a pixel underneath this tolerance value does not influence
    the calculation of the cropping region
    :param target_shape: output image will have shape (target_shape, target_shape)
    :return: cropping regions are determined first per slice by calculating the middle of the most outer "relevant"
    pixels. Then, the average of those cropping centers is built over all slices. A 128-square is placed around the
    mean cropping center and every slice is cropped using this square.
    """
    relevant = image_array > tol
    x0_list = []
    x1_list = []
    y0_list = []
    y1_list = []
    for rel_s in relevant:
        coords = np.argwhere(rel_s)
        x0, y0 = coords.min(axis=0)
        x1, y1 = coords.max(axis=0) + 1  # slices are exclusive at the top
        x0_list.append(x0)
        x1_list.append(x1)
        y0_list.append(y0)
        y1_list.append(y1)
    x_center = int(0.5 * (np.mean(np.array(x0_list)) + np.mean(np.array(x1_list))))
    y_center = int(0.5 * (np.mean(np.array(y0_list)) + np.mean(np.array(y1_list))))
    x_min, x_max = x_center - int(target_shape / 2), x_center + int(target_shape / 2) + target_shape % 2
    y_min, y_max = y_center - int(target_shape / 2), y_center + int(target_shape / 2) + target_shape % 2
    if return_only_cuts:
        return x_min, x_max, y_min, y_max
    else:
        return image_array[:, x_min:x_max, y_min:y_max]


def cut_off(prediction, cut_off_value):
    """
    :param prediction: two-dimensional array with predictions of type float in (0,1)
    :param cut_off_value: threshold for binarization of the UNET's predictions
    :return: binarized prediction along the cutting threshold
    """
    cut_array = np.zeros(prediction.shape)
    cut_array[prediction >= cut_off_value] = 1
    return cut_array


def standardize(img):
    """
    :param img: two-dimensional array
    :return: standardized array
    """
    if np.max(img) == np.min(img):
        return np.zeros(img.shape)
    return (img - np.min(img)) / (np.max(img) - np.min(img))


def standardize_array_per_img(array):
    """
    :param array: array which needs to be standardized
    :return: same array but standardized along the first axis
    """
    re_array = np.array([standardize(a) for a in array])
    return re_array


def gen_maj_pred_of_images(model_list, input_images, cut_off_value):
    """
    :param model_list: segmentation models trained on data generated by the experts
    :param input_images: scan slices from one patient with shape (n, 128, 128) which should be processed by the models
    :param cut_off_value: threshold for binarization of the UNET's predictions
    :return: majority vote prediction of all models
    """
    if len(input_images.shape) == 3:
        expanded = np.expand_dims(input_images, -1)
    else:
        expanded = input_images
    pred_array_list = []
    for model in model_list:
        pred_array = cut_off(model.predict_on_batch(expanded), cut_off_value)
        pred_array_list.append(pred_array)
    all_preds = np.array(pred_array_list)
    pred_sums = np.sum(all_preds, axis=0)
    return (pred_sums>np.floor(len(model_list)/2)).astype(int)


def gen_prediction_list_of_images(model_list, input_images, cut_off_value):
    """
    :param model_list: segmentation models trained on data generated by the experts
    :param input_images: scan slices from one patient with shape (n, 128, 128) which should be processed by the models
    :param cut_off_value: threshold for binarization of the UNET's predictions
    :return: list of predictions of same length as model list
    """
    if len(input_images.shape) == 3:
        expanded = np.expand_dims(input_images, -1)
    else:
        expanded = input_images
    pred_array_list = []
    for model in model_list:
        pred_array = cut_off(model.predict_on_batch(expanded), cut_off_value)
        pred_array_list.append(pred_array)
    return pred_array_list
    
    
def load_weights_for_patient(empty_models, lopo_model_path, all_pat_model_path, patient):
    """

    :param empty_models: a list of Keras models which matches the dimensions of the weights stored in the model paths
    :param lopo_model_path: path of models trained in leave-one-patient-out mode
    :param all_pat_model_path: path of models trained on all patients which is used for reconstruction on new data
    :param patient: ID of the patient the weights should be loaded for
    :return: if existing, lopo-models for the given patient are returned, else models trained on all patients (as a list)
    """
    lopo_models = np.array(sorted(os.listdir(lopo_model_path)))
    all_pat_models = np.array(sorted(os.listdir(all_pat_model_path)))
    lopo_model_indices = np.array([patient in m for m in lopo_models])
    # case lopo model found:
    if np.sum(lopo_model_indices) == 3:
        for idx, model in enumerate(empty_models):
            model.load_weights(os.path.join(lopo_model_path,lopo_models[lopo_model_indices][idx]))
        lopo_model_used = True
    # case no lopo model found --> use all pat weights:
    elif np.sum(lopo_model_indices) == 0:
        for idx, model in enumerate(empty_models):
            model.load_weights(os.path.join(all_pat_model_path,all_pat_models[idx]))
        lopo_model_used = False
    else:
        print(f'found {np.sum(lopo_model_indices)} (more than 0 but less than 3) lopo models')
        exit(0)
    return empty_models, lopo_model_used





